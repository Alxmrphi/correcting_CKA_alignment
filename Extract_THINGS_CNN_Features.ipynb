{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS (CNN) Feature Extraction\n",
    "\n",
    "The THINGS dataset contains `1,854` classes. Each class has `12` sample images. The MEG participants saw the entire dataset, while the fMRI participants only saw a subset of `720` classes. Here, features for the entire dataset are extracted, associated with filenames so that at later stages, neural network features for specific CNN layers can be extracted. We use the `thingsvision` library to extract features and save them. The activations will be stored in the folder specified below (in `save_folder` / `layer_name`).\n",
    "\n",
    "Prior to this step, the THINGS image dataset was renamed to be a folder containing training images. This file structure was copied and replicated in the same directory and subsequently denoted as the validation image folder (no images present, just folder structure). Then, one image from each of the `1,854` classes was randomly selected and moved over to the corresponding volder in the validation images. This resulted in a training and validation folder, which contained a class label subfolder (consistent with PyTorch's `ImageFolder` specification). The training folder contained 11 images per class and the validation folder contained 1 image per class. In the code below, each of these folders are referred to by `images_folder` (via specification of `data_split` to be either `train` or `val`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alxmrphi/anaconda3/envs/cka_alignment/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import thingsvision\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from thingsvision import get_extractor\n",
    "from thingsvision.utils.storing import save_features\n",
    "from thingsvision.utils.data import ImageDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cornet-s' # 'resnet18'\n",
    "data_split = 'val' # 'train\n",
    "source = 'custom' # 'torchvision'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 32\n",
    "\n",
    "images_folder = Path(f'/Users/alxmrphi/Documents/Data/THINGS/images/imgs_{data_split}')\n",
    "save_folder = Path('/Users/alxmrphi/Documents/Data/THINGS/activations') / model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://s3.amazonaws.com/cornet-models/cornet_s-1d3f7974.pth\" to /Users/alxmrphi/.cache/torch/hub/checkpoints/cornet_s-1d3f7974.pth\n",
      "100%|██████████| 408M/408M [00:23<00:00, 18.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "extractor = get_extractor(\n",
    "  model_name=model_name,\n",
    "  source=source,\n",
    "  device=device,\n",
    "  pretrained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (V1): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (nonlin1): ReLU(inplace=True)\n",
       "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (nonlin2): ReLU(inplace=True)\n",
       "    (output): Identity()\n",
       "  )\n",
       "  (V2): CORblock_S(\n",
       "    (conv_input): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (skip): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (norm_skip): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (nonlin1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (nonlin2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (nonlin3): ReLU(inplace=True)\n",
       "    (output): Identity()\n",
       "    (norm1_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm1_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (V4): CORblock_S(\n",
       "    (conv_input): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (norm_skip): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (nonlin1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (nonlin2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (nonlin3): ReLU(inplace=True)\n",
       "    (output): Identity()\n",
       "    (norm1_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm1_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm1_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm1_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (IT): CORblock_S(\n",
       "    (conv_input): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (skip): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (norm_skip): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (nonlin1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (nonlin2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (nonlin3): ReLU(inplace=True)\n",
       "    (output): Identity()\n",
       "    (norm1_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm1_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm2_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (norm3_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (flatten): Flatten()\n",
       "    (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    (output): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.show_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names_resnet18 = [ 'conv1', 'maxpool',\n",
    "                        'layer1.0.conv1', 'layer1.0.conv2', 'layer1.1.conv1', 'layer1.1.conv2',\n",
    "                        'layer2.0.conv1', 'layer2.0.conv2', 'layer2.1.conv1', 'layer2.1.conv2',\n",
    "                        'layer3.0.conv1', 'layer3.0.conv2', 'layer3.1.conv1', 'layer3.1.conv2',\n",
    "                        'layer4.0.conv1', 'layer4.0.conv2', 'layer4.1.conv1', 'layer4.1.conv2',\n",
    "                        'avgpool', 'fc']\n",
    "\n",
    "layer_names_cornet_s = ['V1.conv1', 'V1.conv2', 'V2.conv1', 'V2.conv2', 'V2.conv3',\n",
    "                        'V4.conv1', 'V4.conv2', 'V4.conv3', 'IT.conv1', 'IT.conv2', 'IT.conv3',\n",
    "                        'decoder.avgpool', 'decoder.linear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Creating dataset.\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageDataset(\n",
    "  root=images_folder,\n",
    "  out_path=save_folder,\n",
    "  backend=extractor.get_backend(),\n",
    "  transforms=extractor.get_transformations()\n",
    ")\n",
    "\n",
    "batches = DataLoader(\n",
    "  dataset=dataset,\n",
    "  batch_size=batch_size, \n",
    "  backend=extractor.get_backend()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: V1.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [10:06<00:00, 10.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: V1.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [10:07<00:00, 10.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: V2.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:55<00:00, 10.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: V2.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:57<00:00, 10.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: V2.conv3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:54<00:00, 10.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: V4.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:51<00:00, 10.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: V4.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [36:00<00:00, 37.26s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: V4.conv3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [41:24<00:00, 42.83s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: IT.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [5:09:57<00:00, 320.65s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: IT.conv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:57<00:00, 10.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: IT.conv3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:51<00:00, 10.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: decoder.avgpool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:50<00:00, 10.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n",
      "Processing: decoder.linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 58/58 [09:21<00:00,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Features successfully extracted for all 1854 images in the database.\n",
      "\n",
      "Output directory did not exist. Creating directories to save features...\n",
      "\n",
      "...Features successfully saved to disk.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer_names = layer_names_cornet_s\n",
    "#layer_names = layer_names_resnet18\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    print(f\"Processing: {layer_name}\")\n",
    "\n",
    "    features = extractor.extract_features(\n",
    "    batches=batches,\n",
    "    module_name=layer_name,\n",
    "    flatten_acts=True)\n",
    "\n",
    "    save_path = save_folder / layer_name\n",
    "    save_features(features, out_path=save_path, file_format='npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cka_alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
